{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "9d26fe21-92c3-48fb-8f01-f2c4280c07ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<!-- Styled Box with White Text -->\n",
    "<div style=\"background-color: #1B5162; color: #FFFFFF; padding: 15px; border: 1px solid #004085; border-radius: 0px; text-align: center; margin: 10px 0; font-size: 1.5em;\">\n",
    "  <p style=\"text-align: center; margin: 0px 0; font-size: 1em;\">Coffee Demand Forecasting</p>\n",
    "  <p style=\"text-align: center; margin: 0px 0; font-size: 1.5em;\"><strong>Train machine learning models in SAP Databricks</strong></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fe370f6-ddbd-456c-a808-6f71438ae5d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setting up a Python environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1dac3fd-f009-4f71-88bc-69f7221aad90",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Required Python Packages"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Install Required Python Packages\n",
    "# ---------------------------------------------------\n",
    "\n",
    "%pip install faker -qqqq\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caf88d8a-a903-4815-bfb4-dd8217e6806e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Environment Setup and Library Imports"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Environment Setup and Library Imports\n",
    "# ---------------------------------------------------\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from faker import Faker\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import logging\n",
    "\n",
    "os.environ[\"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\"] = \"true\"\n",
    "mlflow.enable_system_metrics_logging()\n",
    "logging.getLogger(\"mlflow.system_metrics.system_metrics_monitor\").setLevel(logging.ERROR)\n",
    "\n",
    "# Initialize Faker for English data\n",
    "fake = Faker('en_US')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"ℹ️ Library import completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "fb0a5fb5-c993-4a74-ba31-355caf3cb09f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"background-color: #d9edf7; color: #31708f; padding: 20px; border-left: 5px solid #31708f; border-radius: 5px; margin: 20px 0;\">\n",
    "  <strong>\uD83D\uDCA1 Tips and Tricks</strong><br/> \n",
    "  The Databricks Serverless Notebook environment provides frequently used Python packages pre-installed. If users want to install additional Python packages, they can use the %pip command. <br/>\n",
    "  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "2572520d-b1e5-4fbd-a941-ffe9a7af6d0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Databricks Unity Catalog\n",
    "<br/>\n",
    "<div style=\"background-color: #F9F7F4; padding: 5px; border: 2px solid #1B5162; border-radius: 10px;\">\n",
    "<ul>\n",
    "  <li><strong style=\"font-size: 1.2em;\">Unity Catalog's 3-level name space</strong>\n",
    "    <ul>\n",
    "      <li><span style=\"font-size: 1.2em;\">Catalog: A Catalog in Unity Catalog is a top-level container that organizes schemas for logical data management.</span></li>\n",
    "      <li><span style=\"font-size: 1.2em;\">Schema: A Schema is a collection within a catalog that groups related tables, views, and data objects.</span></li>\n",
    "      <li><span style=\"font-size: 1.2em;\">Data Objects: Data objects include tables, views, models, and other entities that store or represent data within a schema.</span></li>\n",
    "    </ul>\n",
    "  </li>  \n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c0f1024-ce82-4a15-82fb-1db235c50dc4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Unity Catalog Configuration"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Unity Catalog Configuration\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Unity Catalog settings\n",
    "catalog_name = \"workspace\"\n",
    "schema_name = \"default\"\n",
    "ml_model_name = \"coffee_demand_forecast_model\"\n",
    "\n",
    "# Create catalog and schema using Spark SQL\n",
    "# spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "# spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "# spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "# spark.sql(f\"USE SCHEMA {schema_name}\")\n",
    "\n",
    "print(f\"ℹ️ Unity Catalog setup completed: {catalog_name}.{schema_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e653d3ce-8eb5-4919-ac13-9f48dc212441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Mockup data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41a4dfa6-95cd-4f1e-b249-d2969f7993b2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Drop Existing Tables"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Drop Existing Tables\n",
    "# ---------------------------------------------------\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS demand_forecast_dataset\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS kna1\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS mara\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS mard\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS t001w\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS vbak\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS vbap\")\n",
    "    print(f\"✅ Tables dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error occurred while dropping tables: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3158238f-c0a1-404e-8a37-4f85a50f802a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Regression Data"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Generate Regression Data\n",
    "# ---------------------------------------------------\n",
    "\n",
    "try:\n",
    "    # Generate regression features for each store-product combination\n",
    "    n_stores = 2000\n",
    "    n_products = 20\n",
    "    n_samples = n_stores * n_products  # 40,000 samples\n",
    "\n",
    "    # Generate regression features with controlled noise for 80% accuracy\n",
    "    X_regression, y_base = make_regression(\n",
    "        n_samples=n_samples,\n",
    "        n_features=12,           # Number of features\n",
    "        n_informative=8,         # Informative features\n",
    "        noise=20,                # Noise level for ~80% accuracy\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Convert features to business-meaningful values\n",
    "    features_df = pd.DataFrame(X_regression)\n",
    "    features_df.columns = [\n",
    "        'store_size_factor',     # Store size factor\n",
    "        'location_traffic',      # Location traffic\n",
    "        'season_factor',         # Seasonal factor\n",
    "        'price_sensitivity',     # Price sensitivity\n",
    "        'promotion_effect',      # Promotion effect\n",
    "        'competitor_density',    # Competitor density\n",
    "        'weather_impact',        # Weather impact\n",
    "        'weekend_factor',        # Weekend factor\n",
    "        'trend_factor',          # Trend factor\n",
    "        'economic_indicator',    # Economic indicator\n",
    "        'customer_satisfaction', # Customer satisfaction\n",
    "        'marketing_spend'        # Marketing spend\n",
    "    ]\n",
    "\n",
    "    # Normalize and convert to integer (0-100 range)\n",
    "    for col in features_df.columns:\n",
    "        # Normalize (0-1)\n",
    "        normalized = (features_df[col] - features_df[col].min()) / (features_df[col].max() - features_df[col].min())\n",
    "        # Scale to 0-100 range and convert to integer\n",
    "        features_df[col] = (normalized * 100).astype(int)\n",
    "\n",
    "    # Assign stores and products\n",
    "    features_df['WERKS'] = [f'P{(i//n_products)+1:04d}' for i in range(n_samples)]\n",
    "    features_df['MATNR'] = [f'C{(i%n_products)+1:05d}' for i in range(n_samples)]\n",
    "\n",
    "    # Calculate actual demand quantity\n",
    "    base_demand = 50 + y_base * 10  # Scale base demand\n",
    "    features_df['actual_demand'] = np.maximum(10, base_demand).astype(int)\n",
    "\n",
    "    print(f\"✅ Regression feature data generated successfully (targeting 80% accuracy): {len(features_df)} records\")\n",
    "    \n",
    "    # Check data distribution\n",
    "    print(f\"\\n\uD83D\uDCCA Target variable (actual_demand) statistics:\")\n",
    "    print(f\"  Mean: {features_df['actual_demand'].mean():.2f}\")\n",
    "    print(f\"  Std Dev: {features_df['actual_demand'].std():.2f}\")\n",
    "    print(f\"  Min: {features_df['actual_demand'].min()}\")\n",
    "    print(f\"  Max: {features_df['actual_demand'].max()}\")\n",
    "    \n",
    "    display(features_df)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error occurred while generating regression feature data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c95e6a97-7290-4098-b14b-03b345c70833",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate SAP Material Master Data - MARA"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Generate SAP Material Master Data - MARA\n",
    "# ---------------------------------------------------\n",
    "\n",
    "try:\n",
    "    # Generate 20 types of coffee beans\n",
    "    coffee_types = [\n",
    "        'Colombia_Supremo', 'Ethiopia_Yirgacheffe', 'Brazil_Santos', 'Kenya_AA',\n",
    "        'Guatemala_Antigua', 'Costa_Rica_Tarrazu', 'Jamaica_Blue_Mountain', 'Hawaii_Kona',\n",
    "        'Peru_Organic', 'Mexico_Chiapas', 'Indonesia_Sumatra', 'Vietnam_Robusta',\n",
    "        'India_Malabar', 'Yemen_Mocha', 'Tanzania_Peaberry', 'Nicaragua_Matagalpa',\n",
    "        'Honduras_Marcala', 'El_Salvador_Pacamara', 'Rwanda_Bourbon', 'Panama_Geisha'\n",
    "    ]\n",
    "\n",
    "    # Generate MARA table data\n",
    "    mara_data = []\n",
    "    for i, coffee in enumerate(coffee_types, 1):\n",
    "        mara_data.append({\n",
    "            'MATNR': f'C{i:05d}',  # Material number\n",
    "            'MAKTX': coffee,  # Material description\n",
    "            'MTART': 'ROH',  # Material type (Raw material)\n",
    "            'MEINS': 'KG',  # Base unit of measure\n",
    "            'MATKL': '1000',  # Material group\n",
    "            'BRGEW': round(random.uniform(0.5, 1.0), 2),  # Gross weight\n",
    "            'NTGEW': round(random.uniform(0.45, 0.95), 2),  # Net weight\n",
    "            'ERSDA': fake.date_between(start_date='-5y', end_date='-1y'),  # Created on\n",
    "            'ERNAM': fake.user_name()  # Created by\n",
    "        })\n",
    "\n",
    "    df_mara = spark.createDataFrame(pd.DataFrame(mara_data))\n",
    "    df_mara.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.MARA\")\n",
    "\n",
    "    print(f\"✅ MARA table created successfully: {len(mara_data)} materials\")\n",
    "    display(df_mara)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error occurred while creating MARA table: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39498271-b1e0-4638-b194-5a58ba77fe3f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate SAP Plant (Store) Data - T001W"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Generate SAP Plant (Store) Data - T001W\n",
    "# ---------------------------------------------------\n",
    "\n",
    "try:\n",
    "    # Generate 2000 store data\n",
    "    plant_data = []\n",
    "    us_states = ['CA', 'NY', 'TX', 'FL', 'IL', 'PA', 'OH', 'GA', 'NC', 'MI', \n",
    "                 'NJ', 'VA', 'WA', 'AZ', 'MA', 'TN', 'IN', 'MO', 'MD', 'WI']\n",
    "    us_cities = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', \n",
    "                 'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', 'San Jose',\n",
    "                 'Austin', 'Jacksonville', 'Fort Worth', 'Columbus', 'Charlotte',\n",
    "                 'San Francisco', 'Indianapolis', 'Seattle', 'Denver', 'Washington']\n",
    "\n",
    "    for i in range(1, 2001):\n",
    "        city = random.choice(us_cities)\n",
    "        state = random.choice(us_states)\n",
    "        plant_data.append({\n",
    "            'WERKS': f'P{i:04d}',  # Plant code\n",
    "            'NAME1': f'{city} Store #{i%100+1}',  # Plant name\n",
    "            'LAND1': 'US',  # Country code\n",
    "            'REGIO': state,  # Region code\n",
    "            'STRAS': fake.street_address(),  # Street address\n",
    "            'PSTLZ': fake.zipcode(),  # Postal code\n",
    "            'ORT01': city,  # City\n",
    "            'TELF1': fake.phone_number(),  # Telephone\n",
    "            'EKORG': '1000',  # Purchasing organization\n",
    "            'VKORG': '1000'  # Sales organization\n",
    "        })\n",
    "\n",
    "    df_t001w = spark.createDataFrame(pd.DataFrame(plant_data))\n",
    "    df_t001w.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.T001W\")\n",
    "\n",
    "    print(f\"✅ T001W table created successfully: {len(plant_data)} plants\")\n",
    "    display(df_t001w)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error occurred while creating T001W table: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "595d3777-b8ab-4868-8a7f-0b92c0443621",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate SAP Sales Order Header - VBAK"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Generate SAP Sales Order Header - VBAK\n",
    "# ---------------------------------------------------\n",
    "\n",
    "try:\n",
    "    # Generate sales data for the past 6 months\n",
    "    start_date = datetime.now() - timedelta(days=180)\n",
    "    vbak_data = []\n",
    "\n",
    "    for i in range(100000):  # 100,000 sales orders\n",
    "        order_date = fake.date_between(start_date=start_date, end_date='today')\n",
    "        vbak_data.append({\n",
    "            'VBELN': f'S{i+1:010d}',  # Sales document number\n",
    "            'ERDAT': order_date,  # Created on\n",
    "            'ERZET': fake.time(),  # Created at\n",
    "            'ERNAM': fake.user_name(),  # Created by\n",
    "            'VBTYP': 'C',  # Sales document category (Order)\n",
    "            'AUART': 'TA',  # Sales document type\n",
    "            'VKORG': '1000',  # Sales organization\n",
    "            'VTWEG': '10',  # Distribution channel\n",
    "            'SPART': '00',  # Division\n",
    "            'VKGRP': f'{random.randint(1, 10):03d}',  # Sales group\n",
    "            'VKBUR': f'{random.randint(1, 50):04d}',  # Sales office\n",
    "            'KUNNR': f'P{random.randint(1, 2000):04d}',  # Customer number (Plant)\n",
    "            'NETWR': round(random.uniform(100, 5000), 2),  # Net value\n",
    "            'WAERK': 'USD'  # Currency\n",
    "        })\n",
    "\n",
    "    df_vbak = spark.createDataFrame(pd.DataFrame(vbak_data))\n",
    "    df_vbak.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.VBAK\")\n",
    "\n",
    "    print(f\"✅ VBAK table created successfully: {len(vbak_data)} sales orders\")\n",
    "    display(df_vbak)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error occurred while creating VBAK table: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1c931ad-4c09-4065-8093-5ee3cfe4486b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate SAP Sales Order Items - VBAP"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Generate SAP Sales Order Items - VBAP\n",
    "# ---------------------------------------------------\n",
    "\n",
    "try:\n",
    "    # Generate VBAP data using features_df\n",
    "    vbap_data = []\n",
    "    vbeln_counter = 1\n",
    "\n",
    "    for idx, row in features_df.iterrows():\n",
    "        # Generate multiple orders for each store-product combination\n",
    "        n_orders = random.randint(5, 20)\n",
    "\n",
    "        for _ in range(n_orders):\n",
    "            # Generate order quantity based on actual demand with variation\n",
    "            base_qty = row['actual_demand']\n",
    "            variation_factor = random.uniform(0.7, 1.3)\n",
    "            order_qty = max(1, int(base_qty * variation_factor))\n",
    "\n",
    "            vbap_data.append({\n",
    "                'VBELN': f'S{vbeln_counter:010d}',  # Sales document number\n",
    "                'POSNR': '000010',  # Item number\n",
    "                'MATNR': row['MATNR'],  # Material number\n",
    "                'MATKL': '1000',  # Material group\n",
    "                'WERKS': row['WERKS'],  # Plant\n",
    "                'LGORT': '1000',  # Storage location\n",
    "                'KWMENG': float(order_qty),  # Order quantity\n",
    "                'VRKME': 'KG',  # Sales unit\n",
    "                'NETPR': round(random.uniform(15, 30), 2),  # Net price\n",
    "                'NETWR': round(order_qty * random.uniform(15, 30), 2),  # Net value\n",
    "                'WAERK': 'USD',  # Currency\n",
    "                'VSTEL': '1000',  # Shipping point\n",
    "                'ROUTE': f'R{random.randint(1, 100):03d}'  # Route\n",
    "            })\n",
    "            vbeln_counter += 1\n",
    "\n",
    "    # Limit to 200,000 records\n",
    "    vbap_data = vbap_data[:200000]\n",
    "\n",
    "    df_vbap = spark.createDataFrame(pd.DataFrame(vbap_data))\n",
    "    df_vbap.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.VBAP\")\n",
    "\n",
    "    print(f\"✅ VBAP table created successfully: {len(vbap_data)} sales items\")\n",
    "    display(df_vbap)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error occurred while creating VBAP table: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4a18b1f-5831-4285-b68c-fd68702b61e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate SAP Inventory Data - MARD"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Generate SAP Inventory Data - MARD\n",
    "# ---------------------------------------------------\n",
    "\n",
    "try:\n",
    "    # Generate inventory data for each store-product combination\n",
    "    mard_data = []\n",
    "\n",
    "    for werks in [f'P{i:04d}' for i in range(1, 2001)]:\n",
    "        for matnr in [f'C{i:05d}' for i in range(1, 21)]:\n",
    "            # Get demand information from features_df\n",
    "            demand_info = features_df[(features_df['WERKS'] == werks) & \n",
    "                                    (features_df['MATNR'] == matnr)]\n",
    "            \n",
    "            if not demand_info.empty:\n",
    "                avg_demand = demand_info['actual_demand'].values[0]\n",
    "                # Current stock with variation\n",
    "                current_stock = max(0, int(avg_demand * random.uniform(0.5, 2.0)))\n",
    "                safety_stock = max(10, int(avg_demand * random.uniform(0.3, 0.7)))\n",
    "            else:\n",
    "                current_stock = random.randint(0, 200)\n",
    "                safety_stock = random.randint(10, 50)\n",
    "                \n",
    "            mard_data.append({\n",
    "                'MATNR': matnr,  # Material number\n",
    "                'WERKS': werks,  # Plant\n",
    "                'LGORT': '1000',  # Storage location\n",
    "                'LABST': float(current_stock),  # Available stock\n",
    "                'UMLME': float(random.randint(0, 20)),  # Stock in transfer\n",
    "                'INSME': float(random.randint(0, 10)),  # Stock in quality inspection\n",
    "                'SPEME': float(random.randint(0, 5)),  # Blocked stock\n",
    "                'EISBE': float(safety_stock),  # Safety stock\n",
    "                'LGPBE': f'L{random.randint(1, 100):03d}'  # Storage bin\n",
    "            })\n",
    "\n",
    "    df_mard = spark.createDataFrame(pd.DataFrame(mard_data))\n",
    "    df_mard.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.MARD\")\n",
    "\n",
    "    print(f\"✅ MARD table created successfully: {len(mard_data)} inventory records\")\n",
    "    display(df_mard)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error occurred while creating MARD table: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "211573e3-50ac-49ca-9707-59bebbf7a12b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data manipulation for data marts for ML analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "f87205b4-d774-4389-8eff-50e0422ba6ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### List of SAP tables used in this demo\n",
    "<br/>\n",
    "<div style=\"background-color: #F9F7F4; padding: 5px; border: 2px solid #1B5162; border-radius: 10px;\">\n",
    "<ul>\n",
    "  <li><strong style=\"font-size: 1.2em;\">MARA – Material Master </strong>\n",
    "    <ul>\n",
    "      <li><span style=\"font-size: 1.2em;\">MARA contains core material master data used across SAP modules, including material number, description, type, base unit of measure, material group, gross and net weight, and creation details. It serves as the central reference for all processes involving materials, ensuring consistency in purchasing, sales, production, and inventory management.</span></li>\n",
    "    </ul>\n",
    "  </li>  \n",
    "</ul>\n",
    "<ul>\n",
    "  <li><strong style=\"font-size: 1.2em;\">T001W – Plant Master</strong>\n",
    "    <ul>\n",
    "      <li><span style=\"font-size: 1.2em;\">T001W contains master data for plants, covering plant codes, names, addresses, country and region codes, telephone numbers, and associated purchasing and sales organizations. It defines key organizational units used in logistics, production, sales, and financial processes across the enterprise.</span></li>\n",
    "    </ul>\n",
    "  </li>  \n",
    "</ul>\n",
    "<ul>\n",
    "  <li><strong style=\"font-size: 1.2em;\">VBAK – Sales Order Header</strong>\n",
    "    <ul>\n",
    "      <li><span style=\"font-size: 1.2em;\">VBAK contains header-level information for sales orders, such as sales document number, creation date/time, document category and type, sales organization, distribution channel, division, customer number, and net order value with currency. It stores data common to all items within an order and supports the overall sales process flow.</span></li>\n",
    "    </ul>\n",
    "  </li>  \n",
    "</ul>\n",
    "<ul>\n",
    "  <li><strong style=\"font-size: 1.2em;\">VBAP – Sales Order Items</strong>\n",
    "    <ul>\n",
    "      <li><span style=\"font-size: 1.2em;\">VBAP contains item-level details for sales orders, linked to VBAK headers, including material numbers, material groups, plant and storage location assignments, ordered quantity, sales unit, net price, net value, currency, shipping point, and route. Each record represents a specific product or service sold and drives delivery, billing, and revenue recognition activities.</span></li>\n",
    "    </ul>\n",
    "  </li>  \n",
    "</ul>\n",
    "<ul>\n",
    "  <li><strong style=\"font-size: 1.2em;\">MARD – Storage Location Stock</strong>\n",
    "    <ul>\n",
    "      <li><span style=\"font-size: 1.2em;\">MARD contains stock information for materials at the plant and storage location level, tracking available stock, stock in transfer, stock under quality inspection, blocked stock, safety stock, and storage bin details. It supports warehouse operations, stock valuation, and material availability checks for procurement and sales.</span></li>\n",
    "    </ul>\n",
    "  </li>  \n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06d30594-07af-4ec0-80cd-20dc77e62b34",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "MARA Table"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : MARA Table\n",
    "# ---------------------------------------------------\n",
    "\n",
    "spark.sql(f\"SELECT * FROM {catalog_name}.{schema_name}.MARA\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1762bb6-b943-4989-9449-6af8be675547",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "T001W Table"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : T001W Table\n",
    "# ---------------------------------------------------\n",
    "\n",
    "spark.sql(f\"SELECT * FROM {catalog_name}.{schema_name}.T001W\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ff1437f-1c2d-45fd-ac36-8cc95990c9e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "VBAK Table"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : VBAK Table\n",
    "# ---------------------------------------------------\n",
    "\n",
    "spark.sql(f\"SELECT * FROM {catalog_name}.{schema_name}.VBAK\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ed7f32c-6ebb-4eb2-a1a0-f5ef89f4396d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "VBAP Table"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : VBAP Table\n",
    "# ---------------------------------------------------\n",
    "\n",
    "spark.sql(f\"SELECT * FROM {catalog_name}.{schema_name}.VBAP\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35d91006-6f57-4f65-91f4-778dd3eca140",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "MARD Table"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : MARD Table\n",
    "# ---------------------------------------------------\n",
    "\n",
    "spark.sql(f\"SELECT * FROM {catalog_name}.{schema_name}.MARD\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa5c5377-75da-4243-bf9f-8d73f3aac524",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Final Demand Forecast Dataset"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Create Final Demand Forecast Dataset\n",
    "# ---------------------------------------------------\n",
    "\n",
    "try:\n",
    "    # Join all data to create final dataset\n",
    "    query_final = f\"\"\"\n",
    "    WITH base_data AS (\n",
    "        SELECT \n",
    "            s.WERKS,\n",
    "            s.MATNR,\n",
    "            s.avg_sales_qty,\n",
    "            s.total_sales_qty,\n",
    "            s.order_count,\n",
    "            s.avg_price,\n",
    "            s.min_qty,\n",
    "            s.max_qty,\n",
    "            s.std_qty,\n",
    "            s.coefficient_variation,\n",
    "            i.LABST as current_stock,\n",
    "            i.EISBE as safety_stock,\n",
    "            (i.LABST + i.UMLME - i.INSME) as total_available,\n",
    "            st.city,\n",
    "            st.region,\n",
    "            st.city_tier,\n",
    "            p.product_name,\n",
    "            p.gross_weight,\n",
    "            p.product_grade\n",
    "        FROM (\n",
    "            SELECT WERKS, MATNR,\n",
    "                AVG(KWMENG) as avg_sales_qty,\n",
    "                SUM(KWMENG) as total_sales_qty,\n",
    "                COUNT(*) as order_count,\n",
    "                AVG(NETPR) as avg_price,\n",
    "                MIN(KWMENG) as min_qty,\n",
    "                MAX(KWMENG) as max_qty,\n",
    "                STDDEV(KWMENG) as std_qty,\n",
    "                CASE WHEN AVG(KWMENG) > 0 THEN STDDEV(KWMENG) / AVG(KWMENG) ELSE 0 END as coefficient_variation\n",
    "            FROM {catalog_name}.{schema_name}.VBAP\n",
    "            GROUP BY WERKS, MATNR\n",
    "        ) s\n",
    "        LEFT JOIN {catalog_name}.{schema_name}.MARD i \n",
    "            ON s.WERKS = i.WERKS AND s.MATNR = i.MATNR\n",
    "        LEFT JOIN (\n",
    "            SELECT WERKS, ORT01 as city, REGIO as region,\n",
    "                CASE \n",
    "                    WHEN ORT01 IN ('New York', 'Los Angeles', 'Chicago') THEN 3\n",
    "                    WHEN ORT01 IN ('Houston', 'Phoenix', 'Philadelphia') THEN 2\n",
    "                    ELSE 1\n",
    "                END as city_tier\n",
    "            FROM {catalog_name}.{schema_name}.T001W\n",
    "        ) st ON s.WERKS = st.WERKS\n",
    "        LEFT JOIN (\n",
    "            SELECT MATNR, MAKTX as product_name, BRGEW as gross_weight,\n",
    "                CASE \n",
    "                    WHEN MAKTX LIKE '%Blue_Mountain%' OR MAKTX LIKE '%Kona%' OR MAKTX LIKE '%Geisha%' THEN 3\n",
    "                    WHEN MAKTX LIKE '%Yirgacheffe%' OR MAKTX LIKE '%AA%' THEN 2\n",
    "                    ELSE 1\n",
    "                END as product_grade\n",
    "            FROM {catalog_name}.{schema_name}.MARA\n",
    "        ) p ON s.MATNR = p.MATNR\n",
    "    )\n",
    "    SELECT DISTINCT\n",
    "        WERKS,\n",
    "        MATNR,\n",
    "        product_name,\n",
    "        city,\n",
    "        region,\n",
    "        city_tier,\n",
    "        product_grade,\n",
    "        avg_sales_qty,\n",
    "        total_sales_qty,\n",
    "        order_count,\n",
    "        avg_price,\n",
    "        min_qty,\n",
    "        max_qty,\n",
    "        COALESCE(std_qty, 0) as std_qty,\n",
    "        COALESCE(coefficient_variation, 0) as coefficient_variation,\n",
    "        COALESCE(current_stock, 0) as current_stock,\n",
    "        COALESCE(safety_stock, 0) as safety_stock,\n",
    "        COALESCE(total_available, 0) as total_available,\n",
    "        gross_weight,\n",
    "        -- Derived features\n",
    "        CASE WHEN current_stock > 0 THEN avg_sales_qty / current_stock ELSE 0 END as stock_turnover,\n",
    "        current_stock - safety_stock as available_stock,\n",
    "        CASE WHEN avg_sales_qty > 0 THEN current_stock / avg_sales_qty ELSE 0 END as days_of_stock,\n",
    "        avg_price / 10 as price_tier,\n",
    "        order_count / 10 as order_frequency_score,\n",
    "        -- Target variable: next order quantity\n",
    "        GREATEST(0, \n",
    "            avg_sales_qty * 1.2 + \n",
    "            safety_stock * 0.5 - \n",
    "            current_stock * 0.8\n",
    "        ) as next_order_qty\n",
    "    FROM base_data\n",
    "    WHERE avg_sales_qty IS NOT NULL\n",
    "    \"\"\"\n",
    "\n",
    "    df_final = spark.sql(query_final)\n",
    "    df_final.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.demand_forecast_dataset\")\n",
    "\n",
    "    print(f\"✅ Final demand forecast dataset created successfully: {df_final.count()} records\")\n",
    "    display(df_final)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error occurred while creating final demand forecast dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "525f43cc-752f-4079-8bb1-07f19733cab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Variables for machine learning models\n",
    "<br/>\n",
    "<div style=\"background-color: #F9F7F4; padding: 5px; border: 2px solid #1B5162; border-radius: 10px;\">\n",
    "<ul>\n",
    "  <li><strong style=\"font-size: 1.2em;\">Target Variable</strong>\n",
    "    <ul>      \n",
    "      <li><span style=\"font-size: 1.2em;\">next_order_qty : The estimated quantity required for the next order, taking into consideration the average sales volume in the past, safety stock, and current inventory.</span></li>\n",
    "    </ul>\n",
    "  </li>  \n",
    "  <li><strong style=\"font-size: 1.2em;\">Feature Variables</strong>\n",
    "    <ul>      \n",
    "      <li><span style=\"font-size: 1.2em;\">The feature variables consist of plant and product identification information (factory code, material name, city/region, grade), sales indicators (average and total sales volume, number of orders, price, volatility), inventory indicators (current and safety stock, available stock, weight), and derived indicators (inventory turnover rate, inventory days, price and order frequency score).</span></li>\n",
    "    </ul>\n",
    "  </li>  \n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ee508bb-6bc2-4c41-8325-1a78c34c01e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdbd8c0a-2c3d-4326-9fae-a29c340e9ae5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "MLflow Setup and Experiment Initialization"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : MLflow Setup and Experiment Initialization\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Unity Catalog and MLflow integration setup\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Get current user information\n",
    "current_user = spark.sql(\"SELECT current_user() AS user\").collect()[0]['user']\n",
    "experiment_name = f\"/Users/{current_user}/coffee_demand_forecast\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Set model registry name\n",
    "model_name = f\"{catalog_name}.{schema_name}.{ml_model_name}\"\n",
    "\n",
    "print(f\"ℹ️ MLflow experiment setup: {experiment_name}\")\n",
    "print(f\"ℹ️ Model registry: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d55bff1-086a-4ed7-a74b-3971a65bd487",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exploratory Data Analysis (EDA) - Basic Statistics and Distributions"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Exploratory Data Analysis (EDA) - Basic Statistics and Distributions\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Load the final dataset for EDA\n",
    "df_eda = spark.sql(f\"SELECT * FROM {catalog_name}.{schema_name}.demand_forecast_dataset\")\n",
    "pdf_eda = df_eda.toPandas()\n",
    "\n",
    "print(f\"\uD83D\uDCCA Dataset Overview:\")\n",
    "print(f\"  - Total Records: {len(pdf_eda):,}\")\n",
    "print(f\"  - Total Features: {len(pdf_eda.columns)}\")\n",
    "print(f\"  - Memory Usage: {pdf_eda.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Basic statistical summary\n",
    "print(f\"\\n\uD83D\uDCC8 Statistical Summary:\")\n",
    "print(pdf_eda.describe())\n",
    "\n",
    "# Data distribution visualization\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "fig.suptitle('Distribution of Key Variables', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Target variable distribution\n",
    "axes[0, 0].hist(pdf_eda['next_order_qty'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Next Order Quantity Distribution')\n",
    "axes[0, 0].set_xlabel('Order Quantity (KG)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Average sales quantity distribution\n",
    "axes[0, 1].hist(pdf_eda['avg_sales_qty'], bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_title('Average Sales Quantity Distribution')\n",
    "axes[0, 1].set_xlabel('Average Sales (KG)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Current stock distribution\n",
    "axes[0, 2].hist(pdf_eda['current_stock'], bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[0, 2].set_title('Current Stock Distribution')\n",
    "axes[0, 2].set_xlabel('Current Stock (KG)')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Order count distribution\n",
    "axes[1, 0].hist(pdf_eda['order_count'], bins=30, alpha=0.7, color='pink', edgecolor='black')\n",
    "axes[1, 0].set_title('Order Count Distribution')\n",
    "axes[1, 0].set_xlabel('Number of Orders')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Average price distribution\n",
    "axes[1, 1].hist(pdf_eda['avg_price'], bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[1, 1].set_title('Average Price Distribution')\n",
    "axes[1, 1].set_xlabel('Average Price (USD)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Safety stock distribution\n",
    "axes[1, 2].hist(pdf_eda['safety_stock'], bins=50, alpha=0.7, color='gold', edgecolor='black')\n",
    "axes[1, 2].set_title('Safety Stock Distribution')\n",
    "axes[1, 2].set_xlabel('Safety Stock (KG)')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# City tier distribution\n",
    "city_tier_counts = pdf_eda['city_tier'].value_counts().sort_index()\n",
    "axes[2, 0].bar(city_tier_counts.index, city_tier_counts.values, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[2, 0].set_title('City Tier Distribution')\n",
    "axes[2, 0].set_xlabel('City Tier')\n",
    "axes[2, 0].set_ylabel('Count')\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Product grade distribution\n",
    "product_grade_counts = pdf_eda['product_grade'].value_counts().sort_index()\n",
    "axes[2, 1].bar(product_grade_counts.index, product_grade_counts.values, alpha=0.7, color='teal', edgecolor='black')\n",
    "axes[2, 1].set_title('Product Grade Distribution')\n",
    "axes[2, 1].set_xlabel('Product Grade')\n",
    "axes[2, 1].set_ylabel('Count')\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Stock turnover distribution\n",
    "axes[2, 2].hist(pdf_eda['stock_turnover'], bins=50, alpha=0.7, color='navy', edgecolor='black')\n",
    "axes[2, 2].set_title('Stock Turnover Distribution')\n",
    "axes[2, 2].set_xlabel('Stock Turnover Ratio')\n",
    "axes[2, 2].set_ylabel('Frequency')\n",
    "axes[2, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Basic distribution analysis completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "155d26c2-5c42-4d00-a15d-d4b4b519fdaf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "EDA - Correlation Analysis and Heatmap"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : EDA - Correlation Analysis and Heatmap\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Select numerical columns for correlation analysis\n",
    "numerical_cols = [\n",
    "    'city_tier', 'product_grade', 'avg_sales_qty', 'order_count',\n",
    "    'avg_price', 'min_qty', 'max_qty', 'std_qty', 'coefficient_variation',\n",
    "    'current_stock', 'safety_stock', 'total_available', 'gross_weight',\n",
    "    'stock_turnover', 'available_stock', 'days_of_stock', 'price_tier',\n",
    "    'order_frequency_score', 'next_order_qty'\n",
    "]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = pdf_eda[numerical_cols].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(16, 14))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "heatmap = sns.heatmap(correlation_matrix, \n",
    "                      mask=mask,\n",
    "                      annot=True, \n",
    "                      cmap='RdYlBu_r', \n",
    "                      center=0,\n",
    "                      square=True,\n",
    "                      fmt='.2f',\n",
    "                      cbar_kws={'shrink': 0.8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated features\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > 0.7:  # High correlation threshold\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i], \n",
    "                correlation_matrix.columns[j], \n",
    "                corr_value\n",
    "            ))\n",
    "\n",
    "print(f\"\\n\uD83D\uDD0D Highly Correlated Feature Pairs (|correlation| > 0.7):\")\n",
    "for feat1, feat2, corr in high_corr_pairs:\n",
    "    print(f\"  - {feat1} ↔ {feat2}: {corr:.3f}\")\n",
    "\n",
    "# Target variable correlations\n",
    "target_correlations = correlation_matrix['next_order_qty'].abs().sort_values(ascending=False)\n",
    "print(f\"\\n\uD83C\uDFAF Features Most Correlated with Target Variable:\")\n",
    "for feature, corr in target_correlations.head(10).items():\n",
    "    if feature != 'next_order_qty':\n",
    "        print(f\"  - {feature}: {corr:.3f}\")\n",
    "\n",
    "print(\"✅ Correlation analysis completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac9c5fbd-45be-4e1c-8944-ea039e02c57d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "EDA - Store and Product Analysis"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : EDA - Store and Product Analysis\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Store performance analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Store and Product Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Top 20 stores by total sales\n",
    "store_sales = pdf_eda.groupby('WERKS')['total_sales_qty'].sum().sort_values(ascending=False).head(20)\n",
    "axes[0, 0].bar(range(len(store_sales)), store_sales.values, alpha=0.7, color='steelblue')\n",
    "axes[0, 0].set_title('Top 20 Stores by Total Sales')\n",
    "axes[0, 0].set_xlabel('Store Rank')\n",
    "axes[0, 0].set_ylabel('Total Sales Quantity (KG)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Product performance analysis\n",
    "product_sales = pdf_eda.groupby('product_name')['total_sales_qty'].sum().sort_values(ascending=False)\n",
    "axes[0, 1].bar(range(len(product_sales)), product_sales.values, alpha=0.7, color='forestgreen')\n",
    "axes[0, 1].set_title('Product Sales Performance')\n",
    "axes[0, 1].set_xlabel('Product Rank')\n",
    "axes[0, 1].set_ylabel('Total Sales Quantity (KG)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Average order quantity by product grade\n",
    "avg_order_by_grade = pdf_eda.groupby('product_grade')['next_order_qty'].mean()\n",
    "axes[0, 2].bar(avg_order_by_grade.index, avg_order_by_grade.values, alpha=0.7, color='orange')\n",
    "axes[0, 2].set_title('Average Order Quantity by Product Grade')\n",
    "axes[0, 2].set_xlabel('Product Grade')\n",
    "axes[0, 2].set_ylabel('Average Order Quantity (KG)')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# City tier analysis\n",
    "city_sales = pdf_eda.groupby('city_tier')['avg_sales_qty'].mean()\n",
    "axes[1, 0].bar(city_sales.index, city_sales.values, alpha=0.7, color='purple')\n",
    "axes[1, 0].set_title('Average Sales by City Tier')\n",
    "axes[1, 0].set_xlabel('City Tier')\n",
    "axes[1, 0].set_ylabel('Average Sales Quantity (KG)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Regional analysis (top 10 regions)\n",
    "region_sales = pdf_eda.groupby('region')['total_sales_qty'].sum().sort_values(ascending=False).head(10)\n",
    "axes[1, 1].bar(range(len(region_sales)), region_sales.values, alpha=0.7, color='red')\n",
    "axes[1, 1].set_title('Top 10 Regions by Sales')\n",
    "axes[1, 1].set_xlabel('Region Rank')\n",
    "axes[1, 1].set_ylabel('Total Sales Quantity (KG)')\n",
    "axes[1, 1].set_xticks(range(len(region_sales)))\n",
    "axes[1, 1].set_xticklabels(region_sales.index, rotation=45, ha='right')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Price vs Sales relationship\n",
    "axes[1, 2].scatter(pdf_eda['avg_price'], pdf_eda['avg_sales_qty'], alpha=0.6, s=20, color='darkblue')\n",
    "axes[1, 2].set_title('Price vs Sales Relationship')\n",
    "axes[1, 2].set_xlabel('Average Price (USD)')\n",
    "axes[1, 2].set_ylabel('Average Sales Quantity (KG)')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed product analysis\n",
    "print(f\"\\n\uD83D\uDCCA Product Performance Summary:\")\n",
    "product_summary = pdf_eda.groupby('product_name').agg({\n",
    "    'avg_sales_qty': 'mean',\n",
    "    'avg_price': 'mean',\n",
    "    'current_stock': 'mean',\n",
    "    'next_order_qty': 'mean'\n",
    "}).round(2).sort_values('avg_sales_qty', ascending=False)\n",
    "\n",
    "print(product_summary.head(10))\n",
    "print(\"✅ Store and product analysis completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fa53eba-1ba6-41bf-8fd7-4ba7f78069a4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "EDA - Inventory and Stock Analysis"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : EDA - Inventory and Stock Analysis\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Inventory analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Inventory and Stock Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Stock level distribution by product grade\n",
    "product_grades = sorted(pdf_eda['product_grade'].unique())\n",
    "stock_data = [pdf_eda[pdf_eda['product_grade'] == grade]['current_stock'] for grade in product_grades]\n",
    "axes[0, 0].boxplot(stock_data, labels=product_grades)\n",
    "axes[0, 0].set_title('Stock Levels by Product Grade')\n",
    "axes[0, 0].set_xlabel('Product Grade')\n",
    "axes[0, 0].set_ylabel('Current Stock (KG)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Safety stock vs current stock\n",
    "axes[0, 1].scatter(pdf_eda['safety_stock'], pdf_eda['current_stock'], alpha=0.6, s=20, color='green')\n",
    "axes[0, 1].plot([0, pdf_eda['safety_stock'].max()], [0, pdf_eda['safety_stock'].max()], 'r--', label='Safety = Current')\n",
    "axes[0, 1].set_title('Safety Stock vs Current Stock')\n",
    "axes[0, 1].set_xlabel('Safety Stock (KG)')\n",
    "axes[0, 1].set_ylabel('Current Stock (KG)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Stock shortage analysis\n",
    "pdf_eda['stock_shortage'] = pdf_eda['current_stock'] < pdf_eda['safety_stock']\n",
    "shortage_by_grade = pdf_eda.groupby('product_grade')['stock_shortage'].mean() * 100\n",
    "axes[0, 2].bar(shortage_by_grade.index, shortage_by_grade.values, alpha=0.7, color='red')\n",
    "axes[0, 2].set_title('Stock Shortage Rate by Product Grade (%)')\n",
    "axes[0, 2].set_xlabel('Product Grade')\n",
    "axes[0, 2].set_ylabel('Shortage Rate (%)')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Days of stock distribution\n",
    "axes[1, 0].hist(pdf_eda['days_of_stock'], bins=50, alpha=0.7, color='brown', edgecolor='black')\n",
    "axes[1, 0].set_title('Days of Stock Distribution')\n",
    "axes[1, 0].set_xlabel('Days of Stock')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Stock turnover by city tier\n",
    "turnover_by_city = pdf_eda.groupby('city_tier')['stock_turnover'].mean()\n",
    "axes[1, 1].bar(turnover_by_city.index, turnover_by_city.values, alpha=0.7, color='teal')\n",
    "axes[1, 1].set_title('Average Stock Turnover by City Tier')\n",
    "axes[1, 1].set_xlabel('City Tier')\n",
    "axes[1, 1].set_ylabel('Stock Turnover Ratio')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Available stock analysis\n",
    "axes[1, 2].scatter(pdf_eda['available_stock'], pdf_eda['next_order_qty'], alpha=0.6, s=20, color='purple')\n",
    "axes[1, 2].set_title('Available Stock vs Next Order Quantity')\n",
    "axes[1, 2].set_xlabel('Available Stock (KG)')\n",
    "axes[1, 2].set_ylabel('Next Order Quantity (KG)')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stock shortage statistics\n",
    "shortage_stats = pdf_eda.groupby(['city_tier', 'product_grade'])['stock_shortage'].agg(['count', 'sum', 'mean']).round(3)\n",
    "shortage_stats.columns = ['Total_Stores', 'Shortage_Count', 'Shortage_Rate']\n",
    "shortage_stats['Shortage_Rate'] = shortage_stats['Shortage_Rate'] * 100\n",
    "\n",
    "print(f\"\\n\uD83D\uDEA8 Stock Shortage Analysis by City Tier and Product Grade:\")\n",
    "print(shortage_stats)\n",
    "\n",
    "print(\"✅ Inventory and stock analysis completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6618f50-b742-4e16-91f3-0db17066d989",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "EDA - Advanced Scatter Plot Matrix and Outlier Analysis"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : EDA - Advanced Scatter Plot Matrix and Outlier Analysis\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Select key features for scatter plot matrix\n",
    "key_features = [\n",
    "    'avg_sales_qty', 'current_stock', 'avg_price', 'order_count',\n",
    "    'stock_turnover', 'days_of_stock', 'next_order_qty'\n",
    "]\n",
    "\n",
    "# Create scatter plot matrix\n",
    "fig, axes = plt.subplots(len(key_features), len(key_features), figsize=(20, 20))\n",
    "fig.suptitle('Scatter Plot Matrix of Key Features', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i in range(len(key_features)):\n",
    "    for j in range(len(key_features)):\n",
    "        if i == j:\n",
    "            # Diagonal: histograms\n",
    "            axes[i, j].hist(pdf_eda[key_features[i]], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            axes[i, j].set_title(f'{key_features[i]}')\n",
    "        else:\n",
    "            # Off-diagonal: scatter plots\n",
    "            sample_size = min(2000, len(pdf_eda))  # Sample for performance\n",
    "            sample_data = pdf_eda.sample(n=sample_size, random_state=42)\n",
    "            axes[i, j].scatter(sample_data[key_features[j]], sample_data[key_features[i]], \n",
    "                             alpha=0.5, s=10, color='darkblue')\n",
    "            \n",
    "        if i == len(key_features) - 1:\n",
    "            axes[i, j].set_xlabel(key_features[j], rotation=45, ha='right')\n",
    "        if j == 0:\n",
    "            axes[i, j].set_ylabel(key_features[i])\n",
    "        \n",
    "        axes[i, j].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Outlier detection using IQR method\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "fig.suptitle('Outlier Detection Analysis (Box Plots)', fontsize=16, fontweight='bold')\n",
    "\n",
    "outlier_features = ['next_order_qty', 'avg_sales_qty', 'current_stock', 'avg_price',\n",
    "                   'order_count', 'stock_turnover', 'days_of_stock', 'coefficient_variation']\n",
    "\n",
    "for idx, feature in enumerate(outlier_features):\n",
    "    row = idx // 4\n",
    "    col = idx % 4\n",
    "    \n",
    "    # Calculate outliers using IQR\n",
    "    Q1 = pdf_eda[feature].quantile(0.25)\n",
    "    Q3 = pdf_eda[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_threshold_low = Q1 - 1.5 * IQR\n",
    "    outlier_threshold_high = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = pdf_eda[(pdf_eda[feature] < outlier_threshold_low) | \n",
    "                       (pdf_eda[feature] > outlier_threshold_high)]\n",
    "    \n",
    "    # Box plot\n",
    "    box_plot = axes[row, col].boxplot(pdf_eda[feature], patch_artist=True)\n",
    "    box_plot['boxes'][0].set_facecolor('lightblue')\n",
    "    axes[row, col].set_title(f'{feature}\\n({len(outliers)} outliers)')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Outlier summary\n",
    "print(f\"\\n\uD83D\uDD0D Outlier Detection Summary (using IQR method):\")\n",
    "for feature in outlier_features:\n",
    "    Q1 = pdf_eda[feature].quantile(0.25)\n",
    "    Q3 = pdf_eda[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_threshold_low = Q1 - 1.5 * IQR\n",
    "    outlier_threshold_high = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = pdf_eda[(pdf_eda[feature] < outlier_threshold_low) | \n",
    "                       (pdf_eda[feature] > outlier_threshold_high)]\n",
    "    \n",
    "    outlier_percentage = (len(outliers) / len(pdf_eda)) * 100\n",
    "    print(f\"  - {feature}: {len(outliers)} outliers ({outlier_percentage:.2f}%)\")\n",
    "\n",
    "print(\"✅ Advanced analysis and outlier detection completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc0f0b0c-7394-4f6b-903c-0fde3fdad9c1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "EDA - Business Insights and Pattern Analysis"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : EDA - Business Insights and Pattern Analysis\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Business insights visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Business Insights and Pattern Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Order frequency vs Average order size\n",
    "pdf_eda['order_size_category'] = pd.cut(pdf_eda['avg_sales_qty'], \n",
    "                                       bins=[0, 50, 100, 150, float('inf')], \n",
    "                                       labels=['Small', 'Medium', 'Large', 'XLarge'])\n",
    "\n",
    "order_freq_analysis = pdf_eda.groupby('order_size_category')['order_count'].mean()\n",
    "axes[0, 0].bar(order_freq_analysis.index, order_freq_analysis.values, alpha=0.7, color='coral')\n",
    "axes[0, 0].set_title('Order Frequency by Order Size Category')\n",
    "axes[0, 0].set_xlabel('Order Size Category')\n",
    "axes[0, 0].set_ylabel('Average Order Count')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Price sensitivity analysis\n",
    "price_segments = pd.cut(pdf_eda['avg_price'], bins=5, labels=['Low', 'Med-Low', 'Medium', 'Med-High', 'High'])\n",
    "price_demand = pdf_eda.groupby(price_segments)['avg_sales_qty'].mean()\n",
    "axes[0, 1].bar(price_demand.index, price_demand.values, alpha=0.7, color='gold')\n",
    "axes[0, 1].set_title('Average Demand by Price Segment')\n",
    "axes[0, 1].set_xlabel('Price Segment')\n",
    "axes[0, 1].set_ylabel('Average Sales Quantity (KG)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Seasonality proxy (using order variability)\n",
    "cv_segments = pd.cut(pdf_eda['coefficient_variation'], \n",
    "                    bins=[0, 0.3, 0.6, 1.0, float('inf')], \n",
    "                    labels=['Stable', 'Moderate', 'Variable', 'Highly Variable'])\n",
    "seasonal_pattern = pdf_eda.groupby(cv_segments)['next_order_qty'].mean()\n",
    "axes[0, 2].bar(seasonal_pattern.index, seasonal_pattern.values, alpha=0.7, color='lightgreen')\n",
    "axes[0, 2].set_title('Order Quantity by Demand Variability')\n",
    "axes[0, 2].set_xlabel('Demand Variability')\n",
    "axes[0, 2].set_ylabel('Average Next Order Quantity (KG)')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Geographic performance heatmap\n",
    "city_performance = pdf_eda.groupby(['region', 'city_tier']).agg({\n",
    "    'avg_sales_qty': 'mean',\n",
    "    'next_order_qty': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "pivot_sales = city_performance.pivot(index='region', columns='city_tier', values='avg_sales_qty')\n",
    "im1 = axes[1, 0].imshow(pivot_sales.values, cmap='YlOrRd', aspect='auto')\n",
    "axes[1, 0].set_title('Average Sales by Region and City Tier')\n",
    "axes[1, 0].set_xlabel('City Tier')\n",
    "axes[1, 0].set_ylabel('Region')\n",
    "axes[1, 0].set_xticks(range(len(pivot_sales.columns)))\n",
    "axes[1, 0].set_xticklabels(pivot_sales.columns)\n",
    "axes[1, 0].set_yticks(range(len(pivot_sales.index)))\n",
    "axes[1, 0].set_yticklabels(pivot_sales.index)\n",
    "plt.colorbar(im1, ax=axes[1, 0], shrink=0.8)\n",
    "\n",
    "# Inventory efficiency analysis\n",
    "pdf_eda['inventory_efficiency'] = pdf_eda['avg_sales_qty'] / (pdf_eda['current_stock'] + 1)\n",
    "efficiency_by_grade = pdf_eda.groupby('product_grade')['inventory_efficiency'].mean()\n",
    "axes[1, 1].bar(efficiency_by_grade.index, efficiency_by_grade.values, alpha=0.7, color='navy')\n",
    "axes[1, 1].set_title('Inventory Efficiency by Product Grade')\n",
    "axes[1, 1].set_xlabel('Product Grade')\n",
    "axes[1, 1].set_ylabel('Inventory Efficiency Ratio')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Demand prediction complexity analysis\n",
    "pdf_eda['prediction_complexity'] = pdf_eda['std_qty'] / (pdf_eda['avg_sales_qty'] + 1)\n",
    "complexity_dist = pdf_eda['prediction_complexity']\n",
    "axes[1, 2].hist(complexity_dist, bins=50, alpha=0.7, color='darkred', edgecolor='black')\n",
    "axes[1, 2].set_title('Demand Prediction Complexity Distribution')\n",
    "axes[1, 2].set_xlabel('Prediction Complexity Score')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate business insights report\n",
    "print(f\"\\n\uD83D\uDCA1 Key Business Insights:\")\n",
    "\n",
    "# Top performing products\n",
    "top_products = pdf_eda.groupby('product_name')['avg_sales_qty'].mean().sort_values(ascending=False).head(5)\n",
    "print(f\"\\n\uD83C\uDFC6 Top 5 Performing Products:\")\n",
    "for product, sales in top_products.items():\n",
    "    print(f\"  - {product}: {sales:.2f} KG average sales\")\n",
    "\n",
    "# Most challenging products to predict\n",
    "challenging_products = pdf_eda.groupby('product_name')['coefficient_variation'].mean().sort_values(ascending=False).head(5)\n",
    "print(f\"\\n\uD83C\uDFAF Most Challenging Products to Predict:\")\n",
    "for product, cv in challenging_products.items():\n",
    "    print(f\"  - {product}: {cv:.3f} coefficient of variation\")\n",
    "\n",
    "# Regional insights\n",
    "regional_performance = pdf_eda.groupby('region').agg({\n",
    "    'avg_sales_qty': 'mean',\n",
    "    'avg_price': 'mean',\n",
    "    'stock_shortage': 'mean'\n",
    "}).round(3)\n",
    "regional_performance['stock_shortage'] = regional_performance['stock_shortage'] * 100\n",
    "\n",
    "print(f\"\\n\uD83C\uDF0D Regional Performance Summary:\")\n",
    "print(regional_performance.head())\n",
    "\n",
    "print(\"✅ Business insights and pattern analysis completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19e6ed7a-7a35-4fba-a32e-f85ef2f5f89e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Loading and Preprocessing"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Data Loading and Preprocessing\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Load data\n",
    "df_ml = spark.sql(f\"SELECT * FROM {catalog_name}.{schema_name}.demand_forecast_dataset\")\n",
    "pdf_ml = df_ml.toPandas()\n",
    "\n",
    "# Select numerical features\n",
    "feature_columns = [\n",
    "    'city_tier', 'product_grade', 'avg_sales_qty', 'order_count',\n",
    "    'avg_price', 'min_qty', 'max_qty', 'std_qty', 'coefficient_variation',\n",
    "    'current_stock', 'safety_stock', 'total_available', 'gross_weight', \n",
    "    'stock_turnover', 'available_stock', 'days_of_stock', 'price_tier',\n",
    "    'order_frequency_score'\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "target_column = 'next_order_qty'\n",
    "\n",
    "# Handle missing values\n",
    "pdf_ml[feature_columns] = pdf_ml[feature_columns].fillna(0)\n",
    "pdf_ml[target_column] = pdf_ml[target_column].fillna(0)\n",
    "\n",
    "# Handle infinite values\n",
    "pdf_ml = pdf_ml.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# Separate features and target\n",
    "X = pdf_ml[feature_columns]\n",
    "y = pdf_ml[target_column]\n",
    "\n",
    "print(f\"ℹ️ Number of features: {len(feature_columns)}\")\n",
    "print(f\"ℹ️ Total samples: {len(X)}\")\n",
    "print(f\"ℹ️ Target variable mean: {y.mean():.2f}\")\n",
    "print(f\"ℹ️ Target variable std: {y.std():.2f}\")\n",
    "\n",
    "# Check data distribution\n",
    "print(f\"\\n\uD83D\uDCCA Feature statistics:\")\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b266577a-b145-4a70-b6ba-ce6edb10078c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Machine Learning model training with MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "f0545722-6991-4c16-9372-a83bccb6dcb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MLflow\n",
    "<br/>\n",
    "<div style=\"background-color: #F9F7F4; padding: 5px; border: 2px solid #1B5162; border-radius: 10px;\">\n",
    "<ul>\n",
    "  <li><strong style=\"font-size: 1.2em;\">What is MLflow?</strong>\n",
    "    <ul>      \n",
    "      <li><span style=\"font-size: 1.2em;\">MLflow is an open source platform for developing models and generative AI applications. It has the following primary components.</span></li>\n",
    "    </ul>\n",
    "  </li>  \n",
    "  <li><strong style=\"font-size: 1.2em;\">MLflow components</strong>\n",
    "    <ul>      \n",
    "      <li><span style=\"font-size: 1.2em;\">Tracking : Allows you to track experiments to record and compare parameters and results.</span></li>\n",
    "      <li><span style=\"font-size: 1.2em;\">Models : Allow you to manage and deploy models from various ML libraries to various model serving and inference platforms.</span></li>\n",
    "      <li><span style=\"font-size: 1.2em;\">Model Registry : Allows you to manage the model deployment process from staging to production, with model versioning and annotation capabilities.</span></li>\n",
    "      <li><span style=\"font-size: 1.2em;\">AI agent evaluation and tracing : Allows you to develop high-quality AI agents by helping you compare, evaluate, and troubleshoot agents.</span></li>\n",
    "      <li><span style=\"font-size: 1.2em;\">Supported Language : MLflow supports Java, Python, R, and REST APIs.</span></li>\n",
    "    </ul>\n",
    "  </li>  \n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd01ade3-cd00-41b3-af98-7717673aeaa4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Splitting and Scaling"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Data Splitting and Scaling\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Train/test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"ℹ️ Training data: {X_train.shape}\")\n",
    "print(f\"ℹ️ Test data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1a04f67-6087-49d6-85ed-566c3ee26f96",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Linear Regression Model Training"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Linear Regression Model Training\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Start MLflow run - Linear Regression\n",
    "with mlflow.start_run(run_name=\"Linear_Regression\") as run:\n",
    "\n",
    "    # Train model\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_train_lr = lr_model.predict(X_train_scaled)\n",
    "    y_pred_test_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "    # Performance evaluation\n",
    "    train_rmse_lr = np.sqrt(mean_squared_error(y_train, y_pred_train_lr))\n",
    "    test_rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_test_lr))\n",
    "    train_r2_lr = r2_score(y_train, y_pred_train_lr)\n",
    "    test_r2_lr = r2_score(y_test, y_pred_test_lr)\n",
    "    train_mae_lr = mean_absolute_error(y_train, y_pred_train_lr)\n",
    "    test_mae_lr = mean_absolute_error(y_test, y_pred_test_lr)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "    mlflow.log_param(\"n_features\", len(feature_columns))\n",
    "    mlflow.log_param(\"n_samples\", len(X_train))\n",
    "\n",
    "    mlflow.log_metric(\"train_rmse\", train_rmse_lr)\n",
    "    mlflow.log_metric(\"test_rmse\", test_rmse_lr)\n",
    "    mlflow.log_metric(\"train_r2\", train_r2_lr)\n",
    "    mlflow.log_metric(\"test_r2\", test_r2_lr)\n",
    "    mlflow.log_metric(\"train_mae\", train_mae_lr)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae_lr)\n",
    "\n",
    "    # Generate model signature\n",
    "    signature = infer_signature(X_train_scaled, y_pred_train_lr)\n",
    "\n",
    "    # Save and register model\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=lr_model,\n",
    "        artifact_path=\"model\",\n",
    "        signature=signature,\n",
    "        registered_model_name=model_name,\n",
    "        input_example=X_train_scaled[:5]\n",
    "    )\n",
    "\n",
    "    lr_run_id = run.info.run_id\n",
    "\n",
    "    print(\"Linear Regression Model Performance:\")\n",
    "    print(f\"ℹ️ Training R²: {train_r2_lr:.4f}\")\n",
    "    print(f\"ℹ️ Test R²: {test_r2_lr:.4f}\")\n",
    "    print(f\"ℹ️ Training RMSE: {train_rmse_lr:.4f}\")\n",
    "    print(f\"ℹ️ Test RMSE: {test_rmse_lr:.4f}\")\n",
    "    print(f\"ℹ️ Training MAE: {train_mae_lr:.4f}\")\n",
    "    print(f\"ℹ️ Test MAE: {test_mae_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f56c2e35-68de-46ea-a061-9faf3cebe8ef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Random Forest Model Training"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Random Forest Model Training\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Start MLflow run - Random Forest\n",
    "with mlflow.start_run(run_name=\"Random_Forest\") as run:\n",
    "\n",
    "    # Train model\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_train_rf = rf_model.predict(X_train_scaled)\n",
    "    y_pred_test_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "    # Performance evaluation\n",
    "    train_rmse_rf = np.sqrt(mean_squared_error(y_train, y_pred_train_rf))\n",
    "    test_rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_test_rf))\n",
    "    train_r2_rf = r2_score(y_train, y_pred_train_rf)\n",
    "    test_r2_rf = r2_score(y_test, y_pred_test_rf)\n",
    "    train_mae_rf = mean_absolute_error(y_train, y_pred_train_rf)\n",
    "    test_mae_rf = mean_absolute_error(y_test, y_pred_test_rf)\n",
    "\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"max_depth\", 10)\n",
    "    mlflow.log_param(\"min_samples_split\", 5)\n",
    "    mlflow.log_param(\"min_samples_leaf\", 2)\n",
    "    mlflow.log_param(\"n_features\", len(feature_columns))\n",
    "    mlflow.log_param(\"n_samples\", len(X_train))\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"train_rmse\", train_rmse_rf)\n",
    "    mlflow.log_metric(\"test_rmse\", test_rmse_rf)\n",
    "    mlflow.log_metric(\"train_r2\", train_r2_rf)\n",
    "    mlflow.log_metric(\"test_r2\", test_r2_rf)\n",
    "    mlflow.log_metric(\"train_mae\", train_mae_rf)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae_rf)\n",
    "\n",
    "    # Feature importance logging\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    # Save feature importance as artifact\n",
    "    feature_importance.to_csv('/tmp/feature_importance.csv', index=False)\n",
    "    mlflow.log_artifact('/tmp/feature_importance.csv')\n",
    "\n",
    "    # Generate model signature\n",
    "    signature = infer_signature(X_train_scaled, y_pred_train_rf)\n",
    "\n",
    "    # Save and register model\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=rf_model,\n",
    "        artifact_path=\"model\",\n",
    "        signature=signature,\n",
    "        registered_model_name=model_name,\n",
    "        input_example=X_train_scaled[:5]\n",
    "    )\n",
    "\n",
    "    rf_run_id = run.info.run_id\n",
    "\n",
    "    print(\"Random Forest Model Performance:\")\n",
    "    print(f\"ℹ️ Training R²: {train_r2_rf:.4f}\")\n",
    "    print(f\"ℹ️ Test R²: {test_r2_rf:.4f}\")\n",
    "    print(f\"ℹ️ Training RMSE: {train_rmse_rf:.4f}\")\n",
    "    print(f\"ℹ️ Test RMSE: {test_rmse_rf:.4f}\")\n",
    "    print(f\"ℹ️ Training MAE: {train_mae_rf:.4f}\")\n",
    "    print(f\"ℹ️ Test MAE: {test_mae_rf:.4f}\")\n",
    "    print(\"\\nTop 10 Important Features:\")\n",
    "    print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a176e4b6-2408-4b17-aac5-b3a4f9c83a1c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Model Performance Comparison and Visualization"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Model Performance Comparison and Visualization\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Create performance comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Random Forest'],\n",
    "    'Train_R2': [train_r2_lr, train_r2_rf],\n",
    "    'Test_R2': [test_r2_lr, test_r2_rf],\n",
    "    'Train_RMSE': [train_rmse_lr, train_rmse_rf],\n",
    "    'Test_RMSE': [test_rmse_lr, test_rmse_rf],\n",
    "    'Train_MAE': [train_mae_lr, train_mae_rf],\n",
    "    'Test_MAE': [test_mae_lr, test_mae_rf],\n",
    "    'Overfitting': [train_r2_lr - test_r2_lr, train_r2_rf - test_r2_rf]\n",
    "})\n",
    "\n",
    "print(\"ℹ️ Model Performance Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# R² score comparison\n",
    "x_pos = np.arange(len(comparison_df['Model']))\n",
    "width = 0.35\n",
    "axes[0, 0].bar(x_pos - width/2, comparison_df['Train_R2'], width, label='Train R²', alpha=0.8)\n",
    "axes[0, 0].bar(x_pos + width/2, comparison_df['Test_R2'], width, label='Test R²', alpha=0.8)\n",
    "axes[0, 0].set_title('R² Score Comparison (Train vs Test)')\n",
    "axes[0, 0].set_ylabel('R² Score')\n",
    "axes[0, 0].set_xticks(x_pos)\n",
    "axes[0, 0].set_xticklabels(comparison_df['Model'])\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE comparison\n",
    "axes[0, 1].bar(x_pos - width/2, comparison_df['Train_RMSE'], width, label='Train RMSE', alpha=0.8)\n",
    "axes[0, 1].bar(x_pos + width/2, comparison_df['Test_RMSE'], width, label='Test RMSE', alpha=0.8)\n",
    "axes[0, 1].set_title('RMSE Comparison (Train vs Test)')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "axes[0, 1].set_xticks(x_pos)\n",
    "axes[0, 1].set_xticklabels(comparison_df['Model'])\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Predicted vs Actual scatter plot (Linear Regression)\n",
    "sample_size = min(1000, len(y_test))\n",
    "sample_idx = np.random.choice(len(y_test), sample_size, replace=False)\n",
    "axes[1, 0].scatter(y_test.iloc[sample_idx], y_pred_test_lr[sample_idx], alpha=0.6, s=10)\n",
    "axes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1, 0].set_title(f'Linear Regression\\nPredicted vs Actual (R²={test_r2_lr:.3f})')\n",
    "axes[1, 0].set_xlabel('Actual')\n",
    "axes[1, 0].set_ylabel('Predicted')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Predicted vs Actual scatter plot (Random Forest)\n",
    "axes[1, 1].scatter(y_test.iloc[sample_idx], y_pred_test_rf[sample_idx], alpha=0.6, s=10)\n",
    "axes[1, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1, 1].set_title(f'Random Forest\\nPredicted vs Actual (R²={test_r2_rf:.3f})')\n",
    "axes[1, 1].set_xlabel('Actual')\n",
    "axes[1, 1].set_ylabel('Predicted')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Overfitting analysis\n",
    "print(f\"\\n\uD83D\uDCCA Overfitting Analysis:\")\n",
    "print(f\"Linear Regression overfitting degree: {train_r2_lr - test_r2_lr:.4f}\")\n",
    "print(f\"Random Forest overfitting degree: {train_r2_rf - test_r2_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45692644-0c03-45d7-93cd-3f978c038f90",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Best Model Selection and Validation"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Best Model Selection and Validation\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Select best model based on test R²\n",
    "if test_r2_lr > test_r2_rf:\n",
    "    best_model = lr_model\n",
    "    best_model_name = \"Linear Regression\"\n",
    "    best_run_id = lr_run_id\n",
    "    best_test_r2 = test_r2_lr\n",
    "    best_predictions = y_pred_test_lr\n",
    "else:\n",
    "    best_model = rf_model\n",
    "    best_model_name = \"Random Forest\"\n",
    "    best_run_id = rf_run_id\n",
    "    best_test_r2 = test_r2_rf\n",
    "    best_predictions = y_pred_test_rf\n",
    "\n",
    "print(f\"ℹ️ Best Model: {best_model_name}\")\n",
    "print(f\"ℹ️ Test R² Score: {best_test_r2:.4f}\")\n",
    "print(f\"ℹ️ Prediction Accuracy: {best_test_r2*100:.1f}%\")\n",
    "print(f\"ℹ️ Run ID: {best_run_id}\")\n",
    "\n",
    "# Prediction accuracy analysis by error range\n",
    "prediction_errors = np.abs(y_test - best_predictions)\n",
    "relative_errors = prediction_errors / (y_test + 1e-8)  # Prevent division by zero\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA Prediction Error Analysis:\")\n",
    "print(f\"Mean Absolute Error: {prediction_errors.mean():.2f}\")\n",
    "print(f\"Mean Relative Error: {relative_errors.mean()*100:.1f}%\")\n",
    "print(f\"Predictions within 10% error: {(relative_errors < 0.1).mean()*100:.1f}%\")\n",
    "print(f\"Predictions within 20% error: {(relative_errors < 0.2).mean()*100:.1f}%\")\n",
    "print(f\"Predictions within 30% error: {(relative_errors < 0.3).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5c15943-d240-4ccb-8a55-5ff3af8c9be1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Model Version Management and Alias Setting"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Model Version Management and Alias Setting\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Create MLflow client\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "try:\n",
    "    # Check all versions of registered model\n",
    "    model_versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "\n",
    "    print(f\"Model '{model_name}' version information:\")\n",
    "    for version in model_versions:\n",
    "        print(f\"ℹ️ Version {version.version}:\")\n",
    "        print(f\"   - Run ID: {version.run_id}\")\n",
    "        print(f\"   - Status: {version.status}\")\n",
    "        \n",
    "    # Find best model version\n",
    "    best_version = None\n",
    "    challenger_version = None\n",
    "    \n",
    "    for mv in model_versions:\n",
    "        if mv.run_id == best_run_id:\n",
    "            best_version = mv.version\n",
    "        elif mv.run_id != best_run_id:\n",
    "            challenger_version = mv.version\n",
    "\n",
    "    if best_version:\n",
    "        # Set 'champion' alias\n",
    "        client.set_registered_model_alias(\n",
    "            name=model_name,\n",
    "            alias=\"champion\",\n",
    "            version=best_version\n",
    "        )\n",
    "\n",
    "        print(f\"✅ 'champion' alias set for model {model_name} version {best_version}\")\n",
    "\n",
    "        # Set 'challenger' alias\n",
    "        if challenger_version:\n",
    "            client.set_registered_model_alias(\n",
    "                name=model_name,\n",
    "                alias=\"challenger\",\n",
    "                version=challenger_version\n",
    "            )\n",
    "            print(f\"✅ 'challenger' alias set for model {model_name} version {challenger_version}\")\n",
    "\n",
    "        # Update model description\n",
    "        client.update_model_version(\n",
    "            name=model_name,\n",
    "            version=best_version,\n",
    "            description=f\"\"\"\n",
    "            Coffee Order Quantity Demand Forecast Model ({best_model_name})\n",
    "            - Model Type: {best_model_name}\n",
    "            - Test R²: {best_test_r2:.4f}\n",
    "            - Prediction Accuracy: {best_test_r2*100:.1f}%\n",
    "            - Training Data: {X_train.shape[0]:,} records\n",
    "            - Number of Features: {X_train.shape[1]}\n",
    "            - Created: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "            - Status: Champion Model\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        print(\"✅ Model description updated successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error occurred while setting model alias: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb9aaec1-ffa5-470c-9735-51fa6a041d05",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Best Model and Create Prediction Function"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Load Best Model and Create Prediction Function\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Load the champion model\n",
    "loaded_model = mlflow.pyfunc.load_model(f\"models:/{model_name}@champion\")\n",
    "\n",
    "print(f\"✅ Champion model loaded successfully: {model_name}@champion\")\n",
    "\n",
    "# Test prediction with sample data\n",
    "sample_data = X_test_scaled[:5]\n",
    "sample_predictions = loaded_model.predict(sample_data)\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA Sample Predictions:\")\n",
    "for i, pred in enumerate(sample_predictions):\n",
    "    actual = y_test.iloc[i]\n",
    "    print(f\"Sample {i+1}: Predicted={pred:.2f}, Actual={actual:.2f}, Error={abs(pred-actual):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3eb01ec-3557-4845-972c-fbf89690f43c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Simulation using champion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31216c09-04a0-4fb1-ad25-a7f6810af6e6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Order Quantity Prediction Simulation"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Cell : Order Quantity Prediction Simulation\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Order quantity prediction simulation for specific stores and products\n",
    "sample_stores = ['P0001', 'P0100', 'P0500', 'P1000', 'P1500']\n",
    "sample_products = ['C00001', 'C00005', 'C00010', 'C00015', 'C00020']\n",
    "\n",
    "print(\"Order Quantity Prediction Simulation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "simulation_results = []\n",
    "\n",
    "for store in sample_stores:\n",
    "    for product in sample_products:\n",
    "        # Get store-product data\n",
    "        store_product_data = pdf_ml[(pdf_ml['WERKS'] == store) & \n",
    "                                   (pdf_ml['MATNR'] == product)]\n",
    "        \n",
    "        if not store_product_data.empty:\n",
    "            # Prepare features\n",
    "            features = store_product_data[feature_columns].values\n",
    "            features_scaled = scaler.transform(features)\n",
    "            \n",
    "            # Prediction\n",
    "            predicted_qty = loaded_model.predict(features_scaled)[0]\n",
    "            \n",
    "            # Current information\n",
    "            current_stock = store_product_data['current_stock'].values[0]\n",
    "            avg_sales = store_product_data['avg_sales_qty'].values[0]\n",
    "            safety_stock = store_product_data['safety_stock'].values[0]\n",
    "            product_name = store_product_data['product_name'].values[0]\n",
    "            city = store_product_data['city'].values[0]\n",
    "            \n",
    "            # Order recommendation\n",
    "            recommended_order = max(0, predicted_qty)\n",
    "            urgency = \"Urgent\" if current_stock < safety_stock else \"Normal\"\n",
    "            \n",
    "            simulation_results.append({\n",
    "                'store': store,\n",
    "                'product': product,\n",
    "                'product_name': product_name,\n",
    "                'city': city,\n",
    "                'current_stock': current_stock,\n",
    "                'safety_stock': safety_stock,\n",
    "                'avg_sales': avg_sales,\n",
    "                'predicted_qty': predicted_qty,\n",
    "                'recommended_order': recommended_order,\n",
    "                'urgency': urgency\n",
    "            })\n",
    "            \n",
    "            print(f\"\\n☕️ {city} {store} - {product} ({product_name})\")\n",
    "            print(f\"  - Current Stock: {current_stock:.0f} KG | Safety Stock: {safety_stock:.0f} KG\")\n",
    "            print(f\"  - Average Sales: {avg_sales:.1f} KG/order\")\n",
    "            print(f\"  - Predicted Order Qty: {predicted_qty:.1f} KG\")\n",
    "            print(f\"  - Recommended Order: {recommended_order:.0f} KG ({urgency})\")\n",
    "\n",
    "# Simulation results summary\n",
    "sim_df = pd.DataFrame(simulation_results)\n",
    "urgent_orders = sim_df[sim_df['urgency'] == 'Urgent']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n\uD83D\uDCCA Order Simulation Summary:\")\n",
    "print(f\"  - Total Analysis Target: {len(sim_df)} store-product combinations\")\n",
    "print(f\"  - Urgent Orders Required: {len(urgent_orders)} cases\")\n",
    "print(f\"  - Average Recommended Order Qty: {sim_df['recommended_order'].mean():.1f} KG\")\n",
    "print(f\"  - Total Recommended Order Qty: {sim_df['recommended_order'].sum():.0f} KG\")\n",
    "\n",
    "if len(urgent_orders) > 0:\n",
    "    print(f\"\\n\uD83D\uDEA8 Stores Requiring Urgent Orders:\")\n",
    "    for _, row in urgent_orders.iterrows():\n",
    "        print(f\"  - {row['city']} {row['store']}: {row['product_name']} ({row['recommended_order']:.0f} KG)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "SAP_DBX_Supply_Chain_Demand_Forecasting",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}